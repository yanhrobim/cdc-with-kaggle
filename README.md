<h1 align="center">
Project CDC With Dataset Kaggle </h1>

<p align="center">
  <a href="https://github.com/YanZN0">
  <img src="https://img.shields.io/badge/GitHub-Yan Robim-181717?logo=github"/>
  </a>
  <img src="https://img.shields.io/badge/Python-3.11-blue?logo=python" />
  <img src="https://img.shields.io/badge/Status-Projeto%20Conclu√≠do-green" />
  <img src="https://img.shields.io/badge/license-MIT-green" />

<p align="center">
  <a href="#-sobre-o-projeto">Sobre</a> ‚Ä¢
  <a href="#-dataset">Dataset</a> ‚Ä¢
  <a href="#-arquitetura-do-fluxo-de-dados">Arquitetura</a> ‚Ä¢
  <a href="#-configura√ß√µes">Configura√ß√µes</a> ‚Ä¢
  <a href="#-instala√ß√£o">Instala√ß√£o de Depend√™ncias</a> ‚Ä¢
  <a href="#-como-executar">Como Executar</a> ‚Ä¢
  <a href="#-licen√ßa">Licen√ßa</a> ‚Ä¢
  <a href="#-considera√ß√µes-finais">Considera√ß√µes Finais</a> ‚Ä¢
  <a href="#-autor">Autor</a> ‚Ä¢
</p>

---

## üìå Sobre o Projeto
Projeto implementa um pipeline de dados baseado em um **ETL Incremental**, com o objetivo de gerar e obter arquivos **CDC (Change Data Capture)** com armazenamento em um Bucket Amazon **S3**. Simulando um processo completo de ingest√£o orientada a **Data Lake**.

Este projeto foi desenvolvido com o objetivo de aplicar conceitos da engenharia de dados desenvolvendo um processo completo de **ETL** orientado a **Data Lake**, fazendo uma **ingest√£o incremental**, transforma√ß√£o com Pandas e integra√ß√£o com **nuvem**. Al√©m disso, juntamente cont√©m o objetivo de servir como fonte de dados brutos para estudos futuros com ferramentas que aplicam limpeza, orquestra√ß√£o, processamento distribu√≠do e automatiza√ß√£o.

## üì¶ Dataset
Os dados utilizados no projeto s√£o de um **([Dataset](https://www.kaggle.com/datasets/teocalvo/teomewhy-loyalty-system))**  do Kaggle disponibilizado por um Streamer. O conjunto de dados se baseia em um Sistema de Pontos, conforme os usu√°rios realizam itera√ß√µes durante as transmiss√µes ganham pontos. Desta forma, acumulam pontos para realiza√ß√£o de trocas na lojinha ou realiza√ß√£o de a√ß√µes na transmiss√£o.

O Dataset √© atualizado a cada 6 horas atrav√©s de um workflow automatizado mantido pelo Streamer, garantindo uma fonte de dados constantemente atualizada.

## üß± Arquitetura do Fluxo de Dados
![image](./Arquitetura.png)

O pipeline √© constru√≠do em Python e aborda princ√≠pios de um ETL, √© composto em tr√™s etapas principais:
* **```Extract``` ‚Äî localizado em: ```./src/connection_and_install_dataset.py```**

* **```Transform``` ‚Äî localizado em: ```./src/create_cdc.py```**

* **```Load``` ‚Äî localizado em: ```./src/connection_s3.py```**

Al√©m disso, cont√©m presente um script orquestrador respons√°vel por executar cada etapa do projeto.
* **Pipeline Orquestrador ‚Äî localizado em:  ```./src/pipeline.py```**

O pipeline automatiza a coleta de dados de um Dataset do Kaggle via API, detecta mudan√ßas a cada coleta (Inserts, Updates e Deletes), comparando os dados mais recentes com os antigos. Ap√≥s a detec√ß√£o das altera√ß√µes os arquivos s√£o gerados com data e hora e em seguida armazenados em um bucket Amazon S3.

Para automatizar a execu√ß√£o do pipeline conforme as atualiza√ß√µes do Dataset, foi criado um sistema de **timer configur√°vel** via arquivo **JSON(```config.json```)**, como podem observar abaixo:
```json
    "timer": {
        "unit": "hours",
        "value": 6
        }
```

Esta padronizado para executar a atualiza√ß√£o de carga h√° 6 horas logo ap√≥s a execu√ß√£o manual. Caso tenha interesse em alterar a periodicidade de atualiza√ß√£o da carga, altere o valor da vari√°vel *timer* e *unit* de acordo com seus interesses em **`config.json`**.

## ‚öôÔ∏è Configura√ß√µes

### Pr√©-Requisitos
* **Python ^3.11**
* **Bucket Amazon S3.**


### Env
Antes da execu√ß√£o do projeto, voc√™ precisa conter um arquivo .env para suas credenciais. Caso voc√™ n√£o tenha muito conhecimento sobre como configurar um arquivo **.env**, no projeto voc√™ pode encontrar um arquivo de exemplo **[```./src/example.env```](https://github.com/YanZN0/cdc-with-kaggle/blob/main/src/example.env).**


#### Cred√™nciais Kaggle

Voc√™ precisa gerar sua chave para se comunicar com a API do Kaggle. Ap√≥s obter sua chave, eu recomendo criar um path no seu ambiente **```/.kaggle/sua-chave.json```**. Desta forma, facilita voc√™ salvar suas cred√™nciais no seu arquivo **.env** na seguintes vari√°veis:
```bash
KAGGLE_USERNAME=path-para-sua-chave-kaggle
KAGGLLE_KEY=path-para-sua-chave-kaggle
```

**Caso n√£o tenha conhecimento de como gerar sua chave API do Kaggle, recomendo este post do Medium. ([Passo a Passo](https://medium.com/@wl8380/unlocking-kaggle-datasets-a-guide-to-obtaining-and-installing-your-api-key-65ca25a7ac7c))**

#### Cred√™nciais S3

Voc√™ precisar√° criar um usu√°rio IAM com acesso ao AWS S3, para que o pipeline interaja e fa√ßa a conex√£o com o bucket S3. As cred√™nciais desse usu√°rio precisar√£o ser salvas no seu arquivo **.env**, nas seguintes vari√°veis:
```
AWS_ACCESS_KEY_ID=sua-chave-amazon
AWS_SECRET_ACCESS_KEY=sua-chave-secreta-amazon
AWS_DEFAULT_REGION=regiao-aws-do-seu-bucket
PATH_FILES=seu-caminho-absoluto/cdc-with-kaggle/src/data/cdc/
```
Acabei adicionado a vari√°vel *AWS_DEFAULT_REGION*, desta forma voc√™ pode acessar a regi√£o do seu Bucket espec√≠fico.

PATH_FILES √© o seu caminho absoluto at√© a pasta CDC

**Caso n√£o tenha conhecimento de como criar um usu√°rio IAM para o acesso ao AWS S3, recomendo este post no Medium. ([Passo a Passo](https://medium.com/@anuradha.kadurugasyaya/create-aws-iam-user-for-s3-bucket-892bae4751fc))**

## üì• Instala√ß√£o

| Ferramenta | Finalidade |
|------------|------------|
| **Boto3**  | Conex√£o com Bucket Amazon S3 |
| **Kaggle** | Conex√£o com API do Kaggle   |
| **Pandas** | Transforma√ß√µes para gerar arquivos CDC |

**Por que um Bucket AWS S3?**

O **Amazon S3** foi escolhido ao projeto por ser um armazenamento simples de manter e que oferece servi√ßos por baixo custo, cont√©m uma insfraestrutura de armazenamento de dados dur√°vel, dispon√≠vel e escal√°vel. Al√©m disso, oferece uma estrutura de dados que √© altamente compat√≠vel com ecossistemas de **Big Data**, como Databricks, Redshift, podendo oferecer a continuidade do projeto.

**Por que Pandas?**

No mercado existe alta disponibilidade de ferramentas para transforma√ß√µes, limpeza e processamento de dados, como o **PySpark (Apache Spark)** que entrega √≥timos servi√ßos mas que n√£o cont√©m **nenhuma semelhan√ßa com este projeto**. Eu optei por utilizar o **Pandas** por sua simplicidade e efici√™ncia com um volume de dados consideravelmente menor, como de um Dataset do Kaggle.

### Instala√ß√£o com **pip**

1. Clone o reposit√≥rio e entre na pasta do projeto.
```bash
git clone https://github.com/YanZN0/cdc-with-kaggle.git

cd cdc-with-kaggle 
```

2. Crie e ative um ambiente virtual.
```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# Ou em Linux/MacOs
python -m venv .venv
source .venv/bin/activate
```

3. Instale as depend√™ncias.
```bash
pip install -r requirements.txt
```


## üõ†Ô∏è Como executar


Antes, de executar o pipeline, √© preciso que voc√™ mude estas configura√ß√µes para conex√£o com o seu **Bucket S3**.
```Python

# ./src/connection_s3.py

bucket="nome-seu-bucket",
file_name=f"caminho-dentro-do-bucket/{filename}"

# No c√≥digo, voc√™ ver√° algo como: table['name'] ap√≥s o caminho desejado, isso acontece pois no meu Bucket eu criei pastas com o nome das tabelas, mas voc√™ n√£o precisa seguir esta estrutura. Apenas remova table['name'].
```

Depois de clonar o reposit√≥rio, instalar as depend√™ncias/ferramentas, e estiver dentro do ambiente virtual, rode:
```bash
cd src

python pipeline.py 
```

Caso voc√™ queria ver a execu√ß√£o do **pipeline** com automatiza√ß√£o, √© preciso que voc√™ espere a pr√≥xima atualiza√ß√£o do ([Dataset](https://www.kaggle.com/datasets/teocalvo/teomewhy-loyalty-system)).

Voc√™ tamb√©m pode executar o **pipeline manualmente** por etapa.

**Extract `./src/connection_and_install_dataset.py`**
```bash
cd src

python connection_and_install_dataset.py
```
Caso queira fazer o ***Extract*** de outro Dataset, altere esta configura√ß√£o em `./src/connection_and_install_dataset.py`.
``` python
install_kaggle_dataset(dataset="user/nome-dataset", path="data/current/")`
```

**Transform `./src/create_cdc`**
```bash
cd src

python create_cdc.py
```
Se caso voc√™ executou o ***Extract*** de outro Dataset, com outras tabelas, voc√™ necessariamente precisa adicionar elas ao `config.json`, para que a etapa de ***Transform*** seja executada sem erros. Siga este modelo para a adi√ß√£o de tabelas no **JSON**.
```json
{
  "sep": ";",
  "name": "nome-da-tabela-dataset",
  "date_field": "coluna-de-data-atualiza√ß√£o",
  "pk": "chave-primaria-tabela"
},
```
**Load `./src/connection_s3.py`**
```bash
cd src

python connection_s3.py
```
## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.


## üßæ Considera√ß√µes Finais

O principal objetivo deste projeto foi resolver meu problema de falta de dados brutos para meus estudos da ferramenta **Databricks**. A partir dos arquivos **CDC** gerados por este pipeline consigo simular cargas reais de dados, o que facilita me aprofundar em uma melhor aprendizagem em pr√°ticas de ingest√£o em camadas. Desta forma preparar o cen√°rio para abordagens futuras com processamento em **batch** e **streaming**.

## üë®‚Äçüíª Autor

Feito Por [Yan Robim](https://github.com/YanZN0).

Email para contato: @yanrobim@gmail.com


